{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Intelligent-Agents\" data-toc-modified-id=\"Intelligent-Agents-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Intelligent Agents</a></div><div class=\"lev2 toc-item\"><a href=\"#Agents-and-Environments\" data-toc-modified-id=\"Agents-and-Environments-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Agents and Environments</a></div><div class=\"lev2 toc-item\"><a href=\"#Good-Behavior:-The-Concept-of-Rationality\" data-toc-modified-id=\"Good-Behavior:-The-Concept-of-Rationality-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Good Behavior: The Concept of Rationality</a></div><div class=\"lev3 toc-item\"><a href=\"#Rationality\" data-toc-modified-id=\"Rationality-121\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Rationality</a></div><div class=\"lev3 toc-item\"><a href=\"#Omniscience,-learning,-and-autonomy\" data-toc-modified-id=\"Omniscience,-learning,-and-autonomy-122\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Omniscience, learning, and autonomy</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Nature-of-Environments\" data-toc-modified-id=\"The-Nature-of-Environments-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>The Nature of Environments</a></div><div class=\"lev3 toc-item\"><a href=\"#Specifying-the-task-environment\" data-toc-modified-id=\"Specifying-the-task-environment-131\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Specifying the task environment</a></div><div class=\"lev3 toc-item\"><a href=\"#Properties-of-task-environments\" data-toc-modified-id=\"Properties-of-task-environments-132\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Properties of task environments</a></div><div class=\"lev2 toc-item\"><a href=\"#The-Structure-of-Agents\" data-toc-modified-id=\"The-Structure-of-Agents-14\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>The Structure of Agents</a></div><div class=\"lev3 toc-item\"><a href=\"#Agent-Programs\" data-toc-modified-id=\"Agent-Programs-141\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Agent Programs</a></div><div class=\"lev2 toc-item\"><a href=\"#Key-Points\" data-toc-modified-id=\"Key-Points-15\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Key Points</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents and Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An **agent** is anything that can be viewed as perceiving its **environment** through **sensors** and acting upon that environment through **actuators**.\n",
    "\n",
    "![](https://i.imgur.com/x79uGFG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A human agent has eyes, ears, and other organs for sensors and hands, legs, vocal tract of actuators.\n",
    "- A robotic agent might have cameras and infrared range finders for sensors and various motors for actuators.\n",
    "- A software agent receives keystrokes, file contents, and network packets as sensory inputs and acts on environment by displaying on the screen, writing files, etc.\n",
    "\n",
    "- An agent's **percept sequence** is the complete history of everything that agent has ever perceived.\n",
    ">An agent's choice of action at any given instant can depend on the entire percept sequence observed to date, but not on anything it hasn't perceived.\n",
    "\n",
    "- Mathematically speaking, we say that an agent's behavior is described by the **agent function** that maps any given percept sequence to an action.\n",
    "- *Internally,* the agent function for an artificial agent will be implemented by an **agent program**. It is important to keep these two ideas distinct.\n",
    ">**The agent function** is an abstract mathematical description, **the agent program** is a concrete implementation, running within some physical system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good Behavior: The Concept of Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **rational agent** is one that does the right thing—conceptually speaking, every entry in the table for the agent function is filled out correctly. Obviously, doing the right thing is better than doing the wrong thing, but what does it mean to do the right thing?\n",
    "\n",
    "- When an agent is plunked down in an environment, it generates a sequence of actions according to the percepts it receives. This sequence of actions causes the environment to go through a sequence of states. If the sequence is desirable, then the agent has performed well. This notion of desirability is captured by a **performance measure** that evaluates any given sequence of environment states.\n",
    "\n",
    ">As a generall rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one thinks the agent should behave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rationality at any given time depends on four things:\n",
    "    - The performance measure that defines the criterion of success.\n",
    "    - The agent's prior knowledge of the environment.\n",
    "    - The actions that the agent can perform.\n",
    "    - The agent's percept sequence to date.\n",
    "    \n",
    "- This leads to a **definition of a rational agent**:\n",
    ">For each possible percept sequence, a rational agent should select an action that is ex-\n",
    "pected to maximize its performance measure, given the evidence provided by the percept\n",
    "sequence and whatever built-in knowledge the agent has.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omniscience, learning, and autonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is a distinction between **rationality** and **omniscience**.\n",
    "    - An omniscient agent knows the *actual* outcome of its actions and can act accordingly; but omniscience is impossible in reality.\n",
    "    \n",
    "- Rationality maximizes *expected* performance, while perfection maximizes *actual* performance.\n",
    "\n",
    "- Doing actions *in order to modify future percepts* is called **information gathering** and is an important part of rationality.\n",
    "\n",
    "- A rational agent should be **autonomous** - it should learn what it can to compensate for partial or incorrect prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nature of Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Task environments** are essentially the *problems* to which rational agents are the *solutions*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying the task environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **PEAS** (Performance, Environment, Actuators, Sensors) description. In designing an\n",
    "agent, the first step must always be to specify the task environment as fully as possible.\n",
    "\n",
    "![](https://i.imgur.com/YT0HXef.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of task environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The range of task environments that might rise in AI is obviously vast.\n",
    "- We can, however, identify a fairly small number of dimensions along which task environments can be categorized.\n",
    "\n",
    "![](https://i.imgur.com/w7Gmlwn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fully observable** vs **Partially observable**\n",
    "    - **Fully observable**: if an agent's sensors give it access to the complete state of the environment at each point in time, then we say that the task environment is fully observable.\n",
    "\n",
    "    - **Partially observable**: if noisy and inaccurate sensors or parts of the state are simply missing from the sensor data, then we say that the task environment is partially observable.\n",
    "\n",
    "\n",
    "- **Single agent** vs **multiagent**\n",
    "\n",
    "\n",
    "- **Deterministic** vs **Stochastic**\n",
    "\n",
    "    - If the next state of the environment is completely determined by the current state and the action executed by the agent, the we say the environment is **deterministic**; otherwise, it is **stochastic**.\n",
    "    \n",
    "    - An environment is **uncertain** if it is not fully observable or not deterministic. \n",
    "    \n",
    "    - **NOTE**: the word “stochastic” generally implies that uncertainty about outcomes is quantified in terms of probabilities; a nondeterministic environment is one in which actions are characterized by their possible outcomes, but no probabilities are attached to them. Nondeterministic environment descriptions are usually associated with performance measures that require the agent to succeed for all possible outcomes of its actions.\n",
    "    \n",
    "- **Episodic** vs **Sequential**\n",
    "    - In an **episodic** task environment, the agent's experience is divided into atomic episodes. In each episode the agent receives a percept and then performs a single action. Crucially, the next episode does not depend on the actions taken in previous episodes. Many classification tasks are episodic. \n",
    "    \n",
    "    - In **Sequential** environments, the current decision could affect all future decisions. Chess and Taxi driving are sequential.\n",
    "    \n",
    "    - Episodic environments are much simpler than sequential environments because the agent does not need to think ahead.\n",
    "\n",
    "\n",
    "- **Discrete** vs **Continuous**\n",
    "    \n",
    "    - The discrete/continuous distinction applies to the **state** of the environment, to the way **time** is handled, and to the **precepts** and **actions** of the agent.\n",
    "    \n",
    "- **Known** vs **unknown**\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/JWitaep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Structure of Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The job of AI is to design an **agent program** that implements the agent function-the mapping from precepts to actions.\n",
    "\n",
    "- **agent=architecture+program**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/fKvuN3Z.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An **agent** is something that perceives and acts in an environment. The agent function for an agent specifies the action taken by the agent in response to any percept sequence.\n",
    "- The **performance measure** evaluates the behavior of the agent in an environment. A rational agent acts so as to maximize the expected value of the performance measure, given the percept sequence it has seen so far.\n",
    "- A **task environment** specification includes the performance measure, the external environment, the actuators, and the sensors. In designing an agent, the first step must always be to specify the task environment as fully as possible.\n",
    "- Task environments vary along several significant dimensions. They can be fully or partially observable, single-agent or multiagent, deterministic or stochastic, episodic or sequential, static or dynamic, discrete or continuous, and known or unknown.\n",
    "- The **agent program** implements the agent function. There exists a variety of basic agent-program designs reflecting the kind of information made explicit and used in the decision process. The designs vary in efficiency, compactness, and flexibility. The appropriate design of the agent program depends on the nature of the environment.\n",
    "- **Simple reflex agents** respond directly to percepts, whereas **model-based reflex agents** maintain internal state to track aspects of the world that are not evident in the current percept. **Goal-based agents** act to achieve their goals, and **utility-based agents** try to maximize their own expected “happiness.”\n",
    "- All agents can improve their performance through **learning**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "189px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
